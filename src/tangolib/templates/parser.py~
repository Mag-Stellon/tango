class Lexer:

    def __init__(self,content):
        self.content=content

    def process(self):
        splitTokenFromContent = TOKEN_REGEX.split(self.content)
        tokenizedContent = []

        varTokenStartBegin = re.compile(r"^%s" % (VAR_TOKEN_START))
        eachInstTokenStartBegin = re.compile(r"^%s(\b)*each" % (INST_TOKEN_START))
        callInstTokenStartBegin = re.compile(r"^%s(\b)* call" % (INST_TOKEN_START))
        endInstTokenStartBegin = re.compile(r"^%s(\b)*end"  % (INST_TOKEN_START))
        
        for elem in splitTokenFromContent :
            if varTokenStartBegin.match(elem) is not None:
                tokenizedContent.append(Variable("OK"))
            elif eachInstTokenStartBegin.match(elem) is not None:
                tokenizedContent.append(Each("OK"))
            elif callInstTokenStartBegin.match(elem) is not None:
                tokenizedContent.append(Call("OK"))
            elif endInstTokenStartBegin.match(elem) is not None:
                tokenizedContent.append(End())
            else:
                tokenizedContent.append(Text("OK"))

        print(splitTokenFromContent)
        print(tokenizedContent)
            
